---
title: "STAT547_hw06"
output: github_document
always_allow_html: yes
---

```{r}
library(tidyverse)
library(testthat)
library(gapminder)
library(broom)
```

# (1) Character data

For this prompt, I have selected some exercises from Hadley Wickham's [R for Data Science](https://r4ds.had.co.nz/strings.html#other-types-of-pattern). I will subdivide this into three sections. (A) Miscellaneous exercises and (B) *Regular expressions* exercises, both from **R for Data Science**, and (C) A practical application of regex.

## (1-A) Miscellaneous

```{r}
#Describe the difference between the sep and collapse arguments to str_c().

str_c("a fox", "is not a bear", sep = ": ") #sep is useful with two independent strings
str_c(c("a fox", "is not a bear"), sep = ": ") #sep not so useful with a vector of strings
str_c(c("a fox", "is not a bear"), collapse = ": ") #collapse puts a vector of strings into one string, and separates by the specified argument
```

```{r}
#What does str_wrap() do? When might you want to use it?

s <- str_c(sentences[[1]], sentences[[2]], sentences[[3]], sep = " ") #Random paragraph

#str_wrap formats the text wrapping of the output
str_wrap(s, width = 30, indent = 5, exdent = 3) %>% writeLines()
```

```{r}
#What does str_trim() do? What’s the opposite of str_trim()?

str_trim("   Here is a poorly formatted sentence. It has a lot of white space.    ", side = "both")
#str_trim removes white space from a string, including tab and new lines.

str_pad("Here is a sentence to which I want to add white space.", width = 100)
#str_pad adds white space (or a specified string) until the width reaches the specified argument.
#I use 100 because it is longer than the sentence's character count
```

## (1-B) Regular Expressions

```{r}
#How would you match the sequence "'\?
x <- "\"\'\\"
str_view(x, "\"\'\\\\")
```

```{r}
#What patterns will the regular expression \\..\\..\\.. match? How would you represent it as a string?

x1 <- c("abc123", ".a.b.c", "......", ".\\.\\.\\")
str_view_all(x1, pattern = "\\..\\..\\..")
#"\\.." as a regex finds a literal dot, then anything. Repeat thrice.
```


```{r}
#How would you match the literal string "$^$"?

str_view("$^$", pattern = "\\$\\^\\$")
#similar to finding a literal dot, need two backslashes
```


```{r}
#Given the corpus of common words in stringr::words, create regular expressions that find all words that:

#Start with “y”.
str_view(words, "^y", match = TRUE)

#End with “x”
str_view(words, "x$", match = TRUE)

#Are exactly three letters long.
str_view(words, "^...$", match = TRUE)

#Have seven letters or more.
str_view(words, "^.......", match = TRUE)
```


```{r}
wordSet <- c("alphabet", "Pangea", "zebra", "STAT547", "escape", "tweed", "bed", "wise", "thing")

#Create regular expressions to find all words that:

#Start with a vowel.
str_view(wordSet, pattern = "^[aeiou]", match = TRUE)

#That only contain consonants. (Hint: thinking about matching “not”-vowels.)
str_view(wordSet, pattern = "^[^aeiou]", match = TRUE)

#End with ed, but not with eed.
str_view(wordSet, pattern = "[^e]ed$", match = TRUE)

#End with ing or ise.
str_view(wordSet, pattern = "(ing)|(ise)$", match = TRUE)

#Empirically verify the rule “i before e except after c”.
str_view(words, pattern = "(cie)|(cei)", match = TRUE)

#Is “q” always followed by a “u”?
str_view(words, pattern = "q[^u]", match = TRUE)

```

```{r}
#Write a regular expression that matches a word if it’s probably written in British English, not American English.

#look for words that end in "re" (instead of "er" in American English)
#exclude words that have vowels before "re"
str_view(words, pattern = "[^aeiou]re$", match = TRUE)
```


```{r}
#Create a regular expression that will match telephone numbers as commonly written in your country

phoneNumbers <- c("1-800-123-4567", "604-123-4567", "911", "0123456789")

str_view(phoneNumbers, pattern = "^\\d\\d\\d\\-\\d\\d\\d\\-\\d\\d\\d\\d$")
```

```{r}
#Describe the equivalents of ?, +, * in {m,n} form.

x2 <- c("aaa", "aab", "aaaabb", "b")

#equivalent to '?'
str_view(x2, "a{0,1}")

#equivalent to '+'
str_view(x2, "b{1,}")

#equivalent to '*'
str_view(x2, "a{0,}")
```


```{r}
#Describe in words what these regular expressions match:

# regex ^.*$
str_view("aa", "^.*$")

# string "\\{.+\\}"
str_view("\\{.+\\}", "\\\\\\{\\.\\+\\\\\\}")

# regex \d{4}-\d{2}-\d{2}
str_view("1234-56-78", "\\d{4}-\\d{2}-\\d{2}")

# string "\\\\{4}"
str_view("\\\\{4}", "\\\\\\\\\\{\\d\\}")

```


```{r}
#Create regular expressions to find all words that

x3 <- c("zxcv", "uiop", "Aei", "apart")

#Start with three consonants.
str_view(x3, "^[^aeiou][^aeiou][^aeiou]")

#Have three or more vowels in a row.
str_view(x3, "[AEIOUaeiou][aeiou][aeiou]")

#Have two or more vowel-consonant pairs in a row.
str_view(x3, "([AEIOUaeiou][^aeiou][aeiou][^aeiou])")

```

```{r}
#Construct regular expressions to match words that:

#Start and end with the same character.
str_view("racecar", "^(.).{0,}\\1$")

#Contain a repeated pair of letters
str_view("church", "(.)(.).{0,}\\1\\2")

#Contain one letter repeated in at least three places
str_view("eleven", "(.).{0,}\\1.{0,}\\1")
```

## (1-C) Application of regex

In this section, I use regex and some `stringr` functions to demonstrate how these could be useful in my research. This exercise has been adapted from *R for Data Science*.

Suppose I want to search through survey responses, a Twitter feed, or some other data set that contain dissimilar character strings. I want to search such a data set for words or phrases that pertain to my domain of research, say for example, "energy", "policy", or "solar". Further, there may be variations of the word, for example, "battery" or "batteries". I may also want to filter out any results that have "lead acid battery", because I'm more interested in other types of batteries.

In the code chunk below, I have created test sentences that may or may not contain my words of interest. I then use regular expressions and some `stringr` functions to demonstrate some useful ways to search the data.

```{r}
sentence_set <- c("Energy policy for battery storage is at a critical juncture.",
          "Wind energy has increased by four-fold in the past decade.",
          "Tesla will scale up production of lithium ion batteries next year.",
          "Is nuclear dead? Researchers say no. Fusion is around the corner.",
          "This sentence has none of the words.",
          "Lead acid battery: This sentence should not have a match.")

#write strings to be read later as regular expressions
words_to_match <- c("[Ee]nergy", "[Pp]olicy", "[Ss]olar", "[Ww]ind", "[^(acid) ][Bb]atter(y|ies)", "[Nn]uclear")

#concatenate word_set into a searchable string separated by '|' i.e. boolean "OR"
(matching_set <- str_c(words_to_match, collapse = "|"))

#parse sentence_set for regular expressions in matching_set
(sentences_with_match <- str_subset(sentence_set, matching_set))

#view matched regular expressions within sentences_with_match
str_view_all(sentences_with_match, matching_set)

#extract regular expressions found in sentences_with_match
(matched_words <- str_extract_all(sentences_with_match, matching_set))

```

*Comments:* Regular expressions are very versatile in detetcing words. Though the example I have used is with a simple set of six sentences, this can easily be scaled up to a larger data set.

The logic used in this example will be applied later in prompt #6.

# (2) Writing functions

For this prompt, I have written functions that do a linear regression and a quadratic regression for the gapminder data set. I will use these later for prompt #6.

The functions takes a vector 'A' and fit a linear or quadratic regression with `lm()` for 'x' and 'y'. `glance()` returns a useful one-row statistical summary from the `broom` package.

```{r}
linearFit <- function(A) {
  fit <- lm(lifeExp ~ I(year - 1952), data = A) %>% 
    glance()
}

quadFit <- function(A) {
  fit <- lm(lifeExp ~ I(year - 1952) + I((year - 1952)^2), data = A) %>% 
    glance()
}

#Test function with a simple case:

r_sq_lm <- lm(lifeExp ~ I(year - 1952), gapminder) %>% glance() %>% select("r.squared")
r_sq_linearFit <- (linearFit(gapminder) %>% select("r.squared"))

r_sq_lm_quad <- lm(lifeExp ~ I(year - 1952) + I((year - 1952)^2), gapminder) %>%
  glance() %>% select("r.squared")
r_sq_quadFit <- (quadFit(gapminder) %>% select("r.squared"))

test_that("Simple case for linear regression works.",
          expect_equal(r_sq_linearFit, r_sq_lm)
          )

test_that("Simple case for quadratic regression works.",
          expect_equal(r_sq_quadFit, r_sq_lm_quad)
          )
```

*Comments:* Although there is a ready-made linear regression function `lm()`, creating a function here that specifically works with the gapminder data set makes the code in prompt #6 more readable.

As for the quadratic regression function, we can work with the same `lm()` function, but instead set the formula to mimic a second-order regression.

Note that I have set the independent variable as (year - 1952), since the gapminder data set begins at 1952. 

# (6) Work with a nested data frame

For this prompt, I use the functions built in prompt #2 to fit regression models to life expectancy data in the 'gapminder' dataframe.

```{r}
#Nest gapminder data by country
gap_nest <- gapminder %>% nest(-country)

#Fit a linear regression to each country
(gap_fit <- gap_nest %>% 
    mutate(fit = map(gap_nest$data, linearFit))
)

#Get desired output (r.squared values for each country) in tibble form
(gap_Rsq <- gap_fit %>% 
    select(country, fit) %>% 
    unnest() %>% 
    select(country, r.squared)
)

#Check for countries with less than 10% r.squared value
(filter(gap_Rsq, r.squared < 0.1))

#Create a character string for countries that match <10% r.squared value
(poor_fits <- filter(gap_Rsq, r.squared < 0.1) %>% 
  select(country) %>%
  unlist() %>% 
  paste(collapse = "|")
)

#Plot the countries with low r.squared values
gap_nest %>% unnest() %>%
  filter(grepl(poor_fits, country)) %>% 
  ggplot(aes(year, lifeExp)) +
    geom_point() +
    geom_smooth(method = lm, se = FALSE) +
    facet_wrap(~country) +
    theme_classic()

```

*Comments:* Nesting of data frames makes it easier to perform some sort of iterative calculation involving whole sub-sets of data. Thus, `purrr` functions go hand-in-hand with `nest()`.

I have created an object 'poor_matches' which contains the names of countries that have very low r.squared values. Similar to prompt #2 above, I use this to search the gapminder country levels to filter for these specific countries in later code. Unlike #2 above though, here I am working with a list-column (from a nested dataframe) and factors (i.e country levels) , so the function calls are different, even though the logic is very similar. Note that I have used the function `base::grepl` which matches a pattern within elements of a character vector.

Next, let's compare quadratic regression to see if the fit improves for countries that had poor fits with linear regression.

```{r}
#Repeat for quadratic regression
gap_fit_quad <- gap_nest %>% mutate(fit = map(gap_nest$data, quadFit))

(gap_Rsq_quad <- gap_fit_quad %>% 
    select(country, fit) %>% 
    unnest() %>% 
    select(country, r.squared))

#Compare r.squared values of quadratic vs. linear regression
gap_compare <- gap_Rsq_quad %>% filter(grepl(poor_fits, country)) %>% 
  left_join(gap_Rsq, by = "country") %>% 
  rename("r.squared\n(quadratic)" = "r.squared.x",
         "r.squared\n(linear)" = "r.squared.y")
gap_compare
```

*Comments:* Quadratic regression significantly improves the fit of several of the countries that had poor fits with linear regression. Note, however that fits are not necessarily "fantastic". Life expectancy generally improves over time (which is essentially a proxy for advancements in healthcare, etc), but sometimes there are other factors at play that drastically impact life expectancy. In particular, Rwanda has a fit that is very poor even with quadratic regression. We note that Rwandan life expectancy indeed was increasing over time until the 1990s. At this point, the Rwandan Genocide and its consequences had a drastic impact on life expectancy in the country, hence the poor fit with data in the latter years of gapminder. All this to say, that we have to be very careful when we make generalizations with data, such as using time as a proxy for all factors that affect life expectancy. That being said, since this is an exercise to sharpen data science skills, so the disclaimer here is that this is by no means intended to be a academic study of life expectancy!

## End of file